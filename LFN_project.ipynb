{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LFN project",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from scipy.sparse import coo_matrix"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:09:59.631794Z",
          "iopub.execute_input": "2024-12-15T15:09:59.632264Z",
          "iopub.status.idle": "2024-12-15T15:09:59.638275Z",
          "shell.execute_reply.started": "2024-12-15T15:09:59.632225Z",
          "shell.execute_reply": "2024-12-15T15:09:59.63691Z"
        },
        "id": "ASHcRYQcQKaV"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "class FastGCNLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim, activation=None, dropout_rate=0.0, **kwargs):\n",
        "        super(FastGCNLayer, self).__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = activation\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.output_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        super(FastGCNLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, features, adj, sampled_nodes, training=False):\n",
        "        # Convert sampled_nodes to int64 to match adj.indices data type\n",
        "        sampled_nodes = tf.cast(sampled_nodes, dtype=tf.int64)\n",
        "\n",
        "        # Filter adjacency matrix rows and columns for sampled nodes\n",
        "        row_mask = tf.reduce_any(adj.indices[:, 0:1] == sampled_nodes, axis=1)\n",
        "        col_mask = tf.reduce_any(adj.indices[:, 1:2] == sampled_nodes, axis=1)\n",
        "        sampled_adj_indices = tf.boolean_mask(adj.indices, tf.logical_and(row_mask, col_mask))\n",
        "        sampled_adj_values = tf.boolean_mask(adj.values, tf.logical_and(row_mask, col_mask))\n",
        "\n",
        "        # Remap node indices to the range 0 to len(sampled_nodes) - 1\n",
        "        unique_nodes, remapped_indices = tf.unique(tf.reshape(sampled_adj_indices, [-1]))\n",
        "\n",
        "        # Explicitly cast tf.range to int64 to match expected data type\n",
        "        node_map = tf.lookup.StaticHashTable(\n",
        "            tf.lookup.KeyValueTensorInitializer(unique_nodes, tf.range(tf.size(unique_nodes), dtype=tf.int64)), # Cast to int64\n",
        "            -1\n",
        "        )\n",
        "        remapped_indices = tf.reshape(node_map.lookup(tf.reshape(sampled_adj_indices, [-1])), tf.shape(sampled_adj_indices))\n",
        "\n",
        "        # Get the actual number of valid sampled nodes\n",
        "        num_valid_nodes = tf.shape(unique_nodes)[0]\n",
        "\n",
        "        sampled_adj = tf.sparse.SparseTensor(\n",
        "            indices=remapped_indices, #Use remapped indices here\n",
        "            values=sampled_adj_values,\n",
        "            # Use num_valid_nodes instead of len(sampled_nodes)\n",
        "            dense_shape=[num_valid_nodes, num_valid_nodes]\n",
        "        )\n",
        "        sampled_adj = tf.sparse.reorder(sampled_adj)\n",
        "\n",
        "        # Extract features for sampled nodes\n",
        "        # Remap sampled_nodes to the new range (0 to num_samples - 1)\n",
        "        # This is crucial for the second layer\n",
        "        sampled_nodes_remapped = node_map.lookup(sampled_nodes)\n",
        "\n",
        "        # Filter out invalid indices (-1)\n",
        "        valid_indices_mask = tf.where(sampled_nodes_remapped >= 0)\n",
        "        sampled_nodes_remapped = tf.gather_nd(sampled_nodes_remapped, valid_indices_mask)\n",
        "\n",
        "        # Gather features for valid indices only\n",
        "        sampled_features = tf.gather(features, sampled_nodes_remapped, validate_indices=False)\n",
        "\n",
        "        # Perform sparse neighborhood aggregation\n",
        "        aggregated_features = tf.sparse.sparse_dense_matmul(sampled_adj, sampled_features)\n",
        "        aggregated_features = tf.matmul(aggregated_features, self.kernel)\n",
        "\n",
        "        # Apply activation and dropout\n",
        "        if self.activation:\n",
        "            aggregated_features = self.activation(aggregated_features)\n",
        "        return self.dropout(aggregated_features, training=training)\n",
        "\n",
        "# FastGCN Model\n",
        "class FastGCN(tf.keras.Model):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):\n",
        "        super(FastGCN, self).__init__()\n",
        "        self.layer1 = FastGCNLayer(hidden_dim, activation=tf.nn.relu, dropout_rate=dropout_rate)\n",
        "        self.layer2 = FastGCNLayer(output_dim, activation=None, dropout_rate=dropout_rate)\n",
        "\n",
        "    def call(self, features, adj, sampled_nodes, training=False):\n",
        "        x = self.layer1(features, adj, sampled_nodes, training=training)\n",
        "        x = self.layer2(x, adj, sampled_nodes, training=training)\n",
        "        return x\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:09:59.941461Z",
          "iopub.execute_input": "2024-12-15T15:09:59.941909Z",
          "iopub.status.idle": "2024-12-15T15:09:59.949125Z",
          "shell.execute_reply.started": "2024-12-15T15:09:59.94187Z",
          "shell.execute_reply": "2024-12-15T15:09:59.947916Z"
        },
        "id": "-ki3ww_lQKaW"
      },
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance Sampling Function\n",
        "def importance_sampling(adj, num_samples):\n",
        "    degrees = tf.sparse.reduce_sum(adj, axis=1) + 1e-5  # Avoid division by zero\n",
        "    probabilities = degrees / tf.reduce_sum(degrees)\n",
        "    sampled_nodes = np.random.choice(adj.shape[0], num_samples, replace=False, p=probabilities.numpy())\n",
        "    return tf.constant(sampled_nodes, dtype=tf.int32)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train(model, features, adj, labels, train_mask, num_epochs=50, num_samples=500, learning_rate=0.01):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Importance sampling\n",
        "            sampled_nodes = importance_sampling(adj, num_samples)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(features, adj, sampled_nodes, training=True)\n",
        "\n",
        "            # Get the valid sampled nodes from the first layer\n",
        "            # Ensure valid_sampled_nodes are less than the size of logits\n",
        "            valid_sampled_nodes = tf.where(sampled_nodes < tf.shape(logits)[0])[:, 0]\n",
        "            valid_sampled_nodes = tf.cast(valid_sampled_nodes, dtype=tf.int32)\n",
        "\n",
        "            # Further filter to ensure they are within the range of logits\n",
        "            valid_indices = tf.where(valid_sampled_nodes < tf.shape(logits)[0])\n",
        "            valid_sampled_nodes = tf.gather_nd(valid_sampled_nodes, valid_indices)\n",
        "\n",
        "            # Compute loss for valid sampled nodes\n",
        "            sampled_labels = tf.gather(labels, valid_sampled_nodes)\n",
        "            loss = loss_fn(sampled_labels, tf.gather(logits, valid_sampled_nodes))\n",
        "\n",
        "        # Backward pass\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy():.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:10:00.252428Z",
          "iopub.execute_input": "2024-12-15T15:10:00.252852Z",
          "iopub.status.idle": "2024-12-15T15:10:00.261134Z",
          "shell.execute_reply.started": "2024-12-15T15:10:00.252794Z",
          "shell.execute_reply": "2024-12-15T15:10:00.259671Z"
        },
        "id": "tIB57fdoQKaW"
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cora():\n",
        "    num_nodes = 2708\n",
        "    num_features = 1433\n",
        "    num_classes = 7\n",
        "\n",
        "    # Simulate feature matrix and labels\n",
        "    features = np.random.rand(num_nodes, num_features).astype(np.float32)\n",
        "    labels = np.random.randint(0, num_classes, size=(num_nodes,), dtype=np.int32)\n",
        "\n",
        "    # Simulate adjacency matrix (sparse graph structure)\n",
        "    row = np.random.randint(0, num_nodes, size=10000)\n",
        "    col = np.random.randint(0, num_nodes, size=10000)\n",
        "    data = np.ones(len(row), dtype=np.float32)\n",
        "    adjacency = coo_matrix((data, (row, col)), shape=(num_nodes, num_nodes))\n",
        "\n",
        "    # Define train mask (e.g., first 140 nodes for training)\n",
        "    train_mask = np.zeros(num_nodes, dtype=bool)\n",
        "    train_mask[:140] = True\n",
        "\n",
        "    return features, labels, adjacency, train_mask\n",
        "\n",
        "# Load data\n",
        "features, labels, adjacency, train_mask = load_cora()\n",
        "\n",
        "# Convert adjacency matrix to TensorFlow SparseTensor\n",
        "adj_tensor = tf.sparse.SparseTensor(\n",
        "    indices=np.vstack((adjacency.row, adjacency.col)).T,\n",
        "    values=adjacency.data,\n",
        "    dense_shape=adjacency.shape\n",
        ")\n",
        "adj_tensor = tf.sparse.reorder(adj_tensor)\n",
        "\n",
        "# Model parameters\n",
        "input_dim = features.shape[1]\n",
        "hidden_dim = 16\n",
        "output_dim = labels.max() + 1\n",
        "\n",
        "# Initialize and train the model\n",
        "model = FastGCN(input_dim, hidden_dim, output_dim, dropout_rate=0.5)\n",
        "train(model, features, adj_tensor, labels, train_mask, num_epochs=50, num_samples=500, learning_rate=0.01)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:10:30.217816Z",
          "iopub.execute_input": "2024-12-15T15:10:30.218257Z",
          "iopub.status.idle": "2024-12-15T15:10:30.405328Z",
          "shell.execute_reply.started": "2024-12-15T15:10:30.218222Z",
          "shell.execute_reply": "2024-12-15T15:10:30.403852Z"
        },
        "id": "hqKobwr5QKaW",
        "outputId": "749abda5-defe-43c3-8b8b-59ca6a32d25e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.1464\n",
            "Epoch 2/50, Loss: 7.9064\n",
            "Epoch 3/50, Loss: 5.3341\n",
            "Epoch 4/50, Loss: 1.9459\n",
            "Epoch 5/50, Loss: 1.9459\n",
            "Epoch 6/50, Loss: 1.9459\n",
            "Epoch 7/50, Loss: 1.9459\n",
            "Epoch 8/50, Loss: 1.9459\n",
            "Epoch 9/50, Loss: 1.9459\n",
            "Epoch 10/50, Loss: 1.9459\n",
            "Epoch 11/50, Loss: 1.9459\n",
            "Epoch 12/50, Loss: 1.9459\n",
            "Epoch 13/50, Loss: 1.9459\n",
            "Epoch 14/50, Loss: 1.9459\n",
            "Epoch 15/50, Loss: 1.9459\n",
            "Epoch 16/50, Loss: 1.9459\n",
            "Epoch 17/50, Loss: 1.9459\n",
            "Epoch 18/50, Loss: 1.9459\n",
            "Epoch 19/50, Loss: 1.9459\n",
            "Epoch 20/50, Loss: 1.9459\n",
            "Epoch 21/50, Loss: 1.9459\n",
            "Epoch 22/50, Loss: 1.9459\n",
            "Epoch 23/50, Loss: 1.9459\n",
            "Epoch 24/50, Loss: 1.9459\n",
            "Epoch 25/50, Loss: 1.9459\n",
            "Epoch 26/50, Loss: 1.9459\n",
            "Epoch 27/50, Loss: 1.9459\n",
            "Epoch 28/50, Loss: 1.9459\n",
            "Epoch 29/50, Loss: 1.9459\n",
            "Epoch 30/50, Loss: 1.9459\n",
            "Epoch 31/50, Loss: 1.9459\n",
            "Epoch 32/50, Loss: 1.9459\n",
            "Epoch 33/50, Loss: 1.9459\n",
            "Epoch 34/50, Loss: 1.9459\n",
            "Epoch 35/50, Loss: 1.9459\n",
            "Epoch 36/50, Loss: 1.9459\n",
            "Epoch 37/50, Loss: 1.9459\n",
            "Epoch 38/50, Loss: 1.9459\n",
            "Epoch 39/50, Loss: 1.9459\n",
            "Epoch 40/50, Loss: 1.9459\n",
            "Epoch 41/50, Loss: 1.9459\n",
            "Epoch 42/50, Loss: 1.9459\n",
            "Epoch 43/50, Loss: 1.9459\n",
            "Epoch 44/50, Loss: 1.9459\n",
            "Epoch 45/50, Loss: 1.9459\n",
            "Epoch 46/50, Loss: 1.9459\n",
            "Epoch 47/50, Loss: 1.9459\n",
            "Epoch 48/50, Loss: 1.9459\n",
            "Epoch 49/50, Loss: 1.9459\n",
            "Epoch 50/50, Loss: 1.9459\n"
          ]
        }
      ],
      "execution_count": 48
    }
  ]
}